Homework 3
================
Adarsh Ramakrishnan

This is my homework 3 solution:

Loading tidyverse library:

``` r
library(tidyverse)
```

    ## -- Attaching packages --------------------------------------- tidyverse 1.3.0 --

    ## v ggplot2 3.3.2     v purrr   0.3.4
    ## v tibble  3.0.3     v dplyr   1.0.2
    ## v tidyr   1.1.2     v stringr 1.4.0
    ## v readr   1.3.1     v forcats 0.5.0

    ## -- Conflicts ------------------------------------------ tidyverse_conflicts() --
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

``` r
library(p8105.datasets)
library(hexbin)
library(patchwork)
```

## Problem 1

``` r
data("instacart")
```

This dataset contants 1384617 rows and xyz columns Observations are
levels of items in order by user. There are user/order variables. – user
id, order id, order day, and order hour. There are also item variables –
name, aisle, department, and some numeric code.

How many aisles are most items from?

``` r
instacart %>% count(aisle) %>% arrange(desc(n))
```

    ## # A tibble: 134 x 2
    ##    aisle                              n
    ##    <chr>                          <int>
    ##  1 fresh vegetables              150609
    ##  2 fresh fruits                  150473
    ##  3 packaged vegetables fruits     78493
    ##  4 yogurt                         55240
    ##  5 packaged cheese                41699
    ##  6 water seltzer sparkling water  36617
    ##  7 milk                           32644
    ##  8 chips pretzels                 31269
    ##  9 soy lactosefree                26240
    ## 10 bread                          23635
    ## # ... with 124 more rows

Let’s make a plot

``` r
instacart %>% count(aisle) %>% filter(n>10000)%>%
mutate ( aisle = factor(aisle), aisle = fct_reorder(aisle, n)) %>%
ggplot(aes(x = aisle, y = n)) + 
geom_point() + 
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

![](p8105_hw3_ar4040_files/figure-gfm/unnamed-chunk-3-1.png)<!-- -->

Let’s make a table\!

``` r
instacart %>%
    filter(aisle %in% c("baking ingredients","dog food care", "packed vegetable fruits"))%>%
  group_by(aisle) %>%
  count(product_name)%>%
  mutate(rank = min_rank(desc(n))) %>%
  filter(rank < 4) %>%
  arrange(aisle, rank) %>%
  knitr::kable()
```

| aisle              | product\_name                                 |   n | rank |
| :----------------- | :-------------------------------------------- | --: | ---: |
| baking ingredients | Light Brown Sugar                             | 499 |    1 |
| baking ingredients | Pure Baking Soda                              | 387 |    2 |
| baking ingredients | Cane Sugar                                    | 336 |    3 |
| dog food care      | Snack Sticks Chicken & Rice Recipe Dog Treats |  30 |    1 |
| dog food care      | Organix Chicken & Brown Rice Recipe           |  28 |    2 |
| dog food care      | Small Dog Biscuits                            |  26 |    3 |

Apples vs ice cream..

``` r
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream"))%>%
  group_by(product_name, order_dow)%>%
  summarise(mean_hour = mean(order_hour_of_day))%>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```

    ## `summarise()` regrouping output by 'product_name' (override with `.groups` argument)

    ## # A tibble: 2 x 8
    ## # Groups:   product_name [2]
    ##   product_name       `0`   `1`   `2`   `3`   `4`   `5`   `6`
    ##   <chr>            <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
    ## 1 Coffee Ice Cream  13.8  14.3  15.4  15.3  15.2  12.3  13.8
    ## 2 Pink Lady Apples  13.4  11.4  11.7  14.2  11.6  12.8  11.9

\#\#Problem 2

Import and clean the accel\_data csv -

``` r
accel_df = read_csv("./data/accel_data.csv")%>% 
janitor::clean_names()%>%
pivot_longer(
    activity_1:activity_1440,
    names_to = "minute_of_day",
    values_to = "activity_count")%>%
mutate(
    weekday_weekend = case_when(
      day == "Saturday" ~ "weekend",
      day == "Sunday" ~ "weekend",
      day == "Monday" ~ "weekday",
      day == "Tuesday" ~ "weekday",
      day == "Wednesday" ~ "weekday",
      day == "Thursday" ~ "weekday",
      day == "Friday" ~ "weekday",
      TRUE      ~ ""
  ))
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_double(),
    ##   day = col_character()
    ## )

    ## See spec(...) for full column specifications.

Describe the dataset-

The cleaned dataset now has 6 variables. The week number and day number
which are both coded as doubles, the day of the week which is a
character variable, the minute of day which represents each minute in a
24 hour day, stored as a character, the activity count which is the
physical activity measured by the accelerometer stored as a double, and
weekday\_weekend which depicts whether the day of the week is a weekday
or weekend stored as a character variable. The final dataset has
`nrow(accel_df)` observations.

Creating a table that aggregated activity count for each of the days

``` r
accel_df%>%
group_by(day_id)%>%
summarize(total_activity_count = sum(activity_count, na.rm = FALSE))%>%
knitr::kable()
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

| day\_id | total\_activity\_count |
| ------: | ---------------------: |
|       1 |              480542.62 |
|       2 |               78828.07 |
|       3 |              376254.00 |
|       4 |              631105.00 |
|       5 |              355923.64 |
|       6 |              307094.24 |
|       7 |              340115.01 |
|       8 |              568839.00 |
|       9 |              295431.00 |
|      10 |              607175.00 |
|      11 |              422018.00 |
|      12 |              474048.00 |
|      13 |              423245.00 |
|      14 |              440962.00 |
|      15 |              467420.00 |
|      16 |              685910.00 |
|      17 |              382928.00 |
|      18 |              467052.00 |
|      19 |              371230.00 |
|      20 |              381507.00 |
|      21 |              468869.00 |
|      22 |              154049.00 |
|      23 |              409450.00 |
|      24 |                1440.00 |
|      25 |              260617.00 |
|      26 |              340291.00 |
|      27 |              319568.00 |
|      28 |              434460.00 |
|      29 |              620860.00 |
|      30 |              389080.00 |
|      31 |                1440.00 |
|      32 |              138421.00 |
|      33 |              549658.00 |
|      34 |              367824.00 |
|      35 |              445366.00 |

Making a plot of 24 hour activity time courses for the different days.

``` r
accel_df%>%
group_by(day_id)%>%
ggplot(aes(x = minute_of_day, y = activity_count, group = day_id, color = day))+
geom_line(alpha = 0.5)+
geom_smooth(aes(group = day_id))
```

    ## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'

    ## Warning: Computation failed in `stat_smooth()`:
    ## NA/NaN/Inf in foreign function call (arg 3)

![](p8105_hw3_ar4040_files/figure-gfm/unnamed-chunk-8-1.png)<!-- -->

\#Problem 3

load the dataset

``` r
data("ny_noaa")
```

clean the dataset

``` r
ny_noaa_df = ny_noaa%>% 
janitor::clean_names()%>% 
separate(date, into = c("year","month","day"), convert = TRUE)%>%
mutate(prcp = as.numeric(prcp) / 10,
       snow = as.numeric(snow),
       snwd = as.numeric(snwd),
       tmin = as.numeric(tmin) / 10,
       tmax = as.numeric(tmax) / 10)
```

the ny\_noaa data has been cleaned such that the date has separated into
year, month, and day. The temperatures have been converted to degrees
celsius from tenths of degrees celsius and all of the precipitation and
snowfall have been converted to mm units.

Now to look at the most common values of snowfall

``` r
ny_noaa_df%>%
  ggplot(aes(x=snow))+
  geom_histogram(na.rm = TRUE)
```

    ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.

![](p8105_hw3_ar4040_files/figure-gfm/unnamed-chunk-11-1.png)<!-- -->

From the histogram above, it is clear that the most common value by far
is 0. This is because it makes sense that it only snows a few days in a
year and hence there would be no snowfall recorded most of the year.

Make a two-panel plot showing the average max temperature in January and
in July in each station across years. Is there any observable /
interpretable structure? Any outliers?

``` r
ny_noaa_df%>%
na.omit()%>%
filter(month %in% c(1,7))%>%
group_by(id,year,month)%>%
summarize(avg_tmax = mean(tmax))%>%
ggplot(  aes(x = year, y = avg_tmax, group = id, color = id) ) +
geom_point()+
geom_path()+
facet_grid(. ~  month)+
labs(title = "Average maximum mean temperature for January and July across stations and years", x = "year", y = "average maximum temperature (degrees C)") +
theme(legend.position = "none") 
```

    ## `summarise()` regrouping output by 'id', 'year' (override with `.groups` argument)

![](p8105_hw3_ar4040_files/figure-gfm/unnamed-chunk-12-1.png)<!-- -->

Make a two-panel plot showing (i) tmax vs tmin for the full dataset
(note that a scatterplot may not be the best option); and (ii) make a
plot showing the distribution of snowfall values greater than 0 and less
than 100 separately by year.

First, let’s make the tmax vs tmin plot for the full dataset

``` r
plot1_tmax_tmin = ny_noaa_df%>%
na.omit()%>%
ggplot(  aes(x = tmin, y = tmax) )+
geom_hex()
```

Second, let’s make a plot showing the distribution of snowfall values
greater than 0 and less than 100 separately by year.

``` r
plot2_snowfall = ny_noaa_df%>%
na.omit()%>%
filter(snow > 0 & snow < 100)%>%
ggplot(  aes(x = year, y = snow) ) +
geom_smooth()
```

Use patchwork to stitch the two plots togther into a 2-panel plot

``` r
plot1_tmax_tmin + plot2_snowfall
```

    ## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'

![](p8105_hw3_ar4040_files/figure-gfm/unnamed-chunk-15-1.png)<!-- -->
