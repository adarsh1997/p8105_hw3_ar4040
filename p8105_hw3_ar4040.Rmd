---
title: "Homework 3"
author: Adarsh Ramakrishnan
output: github_document
---

This is my homework 3 solution:


Loading tidyverse library:

```{r libraries}
library(tidyverse)
library(p8105.datasets)
library(hexbin)
data("instacart")
```


## Problem 1

```{r}
data("instacart")
```

This dataset contants `r nrow(instacart)` rows and xyz columns
Observations are levels of items in order by user. There are user/order variables. -- user id, order id, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric code. 

How many aisles are most items from?

```{r}
instacart %>% count(aisle) %>% arrange(desc(n))
```

Let's make a plot

```{r}
instacart %>% count(aisle) %>% filter(n>10000)%>%
mutate ( aisle = factor(aisle), aisle = fct_reorder(aisle, n)) %>%
ggplot(aes(x = aisle, y = n)) + 
geom_point() + 
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

Let's make a table!

```{r}
instacart %>%
    filter(aisle %in% c("baking ingredients","dog food care", "packed vegetable fruits"))%>%
  group_by(aisle) %>%
  count(product_name)%>%
  mutate(rank = min_rank(desc(n))) %>%
  filter(rank < 4) %>%
  arrange(aisle, rank) %>%
  knitr::kable()
```

Apples vs ice cream..

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream"))%>%
  group_by(product_name, order_dow)%>%
  summarise(mean_hour = mean(order_hour_of_day))%>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```


##Problem 2

Import and clean the accel_data csv - 
```{r}
accel_df = read_csv("./data/accel_data.csv")%>% 
janitor::clean_names()%>%
pivot_longer(
    activity_1:activity_1440,
    names_to = "minute_of_day",
    values_to = "activity_count")%>%
mutate(
    weekday_weekend = case_when(
      day == "Saturday" ~ "weekend",
      day == "Sunday" ~ "weekend",
      day == "Monday" ~ "weekday",
      day == "Tuesday" ~ "weekday",
      day == "Wednesday" ~ "weekday",
      day == "Thursday" ~ "weekday",
      day == "Friday" ~ "weekday",
      TRUE      ~ ""
  ))
```

Creating a table that aggregated activity count for each of the days 

```{r}
accel_df%>%
group_by(day_id)%>%
summarize(total_activity_count = sum(activity_count, na.rm = FALSE))%>%
knitr::kable()
  
```
Making a plot of 24 hour activity time courses for the different days.

```{r}
accel_df%>%
group_by(day_id)%>%
ggplot(aes(x = minute_of_day, y = activity_count, group = day_id, color = day))+
geom_line(alpha = 0.5)+
geom_smooth(aes(group = day))
```

data wrangling - 
1) go from wide format to long format - activity count, minute of the day
2)straightforward
3)useful names for activity count and minute
4)mutate - weekday and weekend
5)variable classes - num, char, factor (is order reasonable)

point 2-
- table with 35 day, total activity count for those days 
- go with week number and number of days
(group by and summarize)
- what jumps out at you looking at the dataset

point 3-
geom_line() could be useful. Linked lines overlapping each other. 35 lines in the middle of the plot 
aes(color = day of week)

potential issues - 
for the table, day of week would by default be in alphabetical order. May need to use factors to order it correctly 

for tidying, is minute of day in numeric format or some other format? Make sure it is numeric. Later on, the plots need to show what you want to show and be in right order

#Problem 3

load the dataset

```{r}
data("ny_noaa")
```

clean the dataset

```{r}
ny_noaa_df = ny_noaa%>% 
janitor::clean_names()%>% 
separate(date, into = c("year","month","day"), convert = TRUE)%>%
mutate(prcp = as.numeric(prcp) / 10,
       snow = as.numeric(snow),
       snwd = as.numeric(snwd),
       tmin = as.numeric(tmin) / 10,
       tmax = as.numeric(tmax) / 10)
```

the ny_noaa data has been cleaned such that the date has separated into year, month, and day. The temperatures have been converted to degrees celsius from tenths of degrees celsius and all of the precipitation and snowfall have been converted to mm units. 

Now to look at the most common values of snowfall

```{r}
ny_noaa_df%>%
  ggplot(aes(x=snow))+
  geom_histogram(na.rm = TRUE)
```

From the histogram above, it is clear that the most common value by far is 0. This is because it makes sense that it only snows a few days in a year and hence there would be no snowfall recorded most of the year.

Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

```{r}
ny_noaa_df%>%
na.omit()%>%
filter(month %in% c(1,7))%>%
group_by(id,year,month)%>%
summarize(avg_tmax = mean(tmax))%>%
ggplot(  aes(x = year, y = avg_tmax, group = id, color = id) ) +
geom_point()+
geom_smooth()+
facet_grid(. ~  month)+
labs(title = "Average maximum mean temperature for January and July across stations and years", x = "year", y = "average maximum temperature (degrees C)") +
theme(legend.position = "none") 
      
```

Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

First, let's make the tmax vs tmin plot for the full dataset

```{r}
plot1_tmax_tmin = ny_noaa_df%>%
na.omit()%>%
ggplot(  aes(x = tmin, y = tmax) )+
geom_hex()
  
```


point 1
- seperate year, month, and day. No need to convert month to descriptor 
- snowfall units should be converted to good units. degrees C perhaps

point 2 
- data manipulation followed by plotting 
- group by and summarize
- group by station, year, month
- summarize to get average max temperature 
- filter Jan and July (doesn't matter which order)
- use count to look at common snowball values
- plot has lot of lines stacked up on each other.
- Use facet to get two panel plot.

point 3
- tmax vs tmin - contour, bin, or hex plot would be better
- second plot, filter by those values, use boxplot, ridgeplots et c to show distribution 

