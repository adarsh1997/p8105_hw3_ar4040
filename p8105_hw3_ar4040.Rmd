---
title: "Homework 3"
author: Adarsh Ramakrishnan
output: github_document
---

This is my homework 3 solution:


Loading tidyverse library:

```{r libraries}
library(tidyverse)
library(p8105.datasets)
library(hexbin)
library(patchwork)
```


## Problem 1

```{r}
data("instacart")
```

This dataset contants `r nrow(instacart)` rows and xyz columns
Observations are levels of items in order by user. There are user/order variables. -- user id, order id, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric code. 

How many aisles are most items from?

```{r}
instacart %>% count(aisle) %>% arrange(desc(n))
```

Let's make a plot

```{r}
instacart %>% count(aisle) %>% filter(n>10000)%>%
mutate ( aisle = factor(aisle), aisle = fct_reorder(aisle, n)) %>%
ggplot(aes(x = aisle, y = n)) + 
geom_point() + 
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

Let's make a table!

```{r}
instacart %>%
    filter(aisle %in% c("baking ingredients","dog food care", "packed vegetable fruits"))%>%
  group_by(aisle) %>%
  count(product_name)%>%
  mutate(rank = min_rank(desc(n))) %>%
  filter(rank < 4) %>%
  arrange(aisle, rank) %>%
  knitr::kable()
```

Apples vs ice cream..

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream"))%>%
  group_by(product_name, order_dow)%>%
  summarise(mean_hour = mean(order_hour_of_day))%>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```


##Problem 2

Import and clean the accel_data csv - 
```{r}
accel_df = read_csv("./data/accel_data.csv")%>% 
janitor::clean_names()%>%
pivot_longer(
    activity_1:activity_1440,
    names_to = "minute_of_day",
    values_to = "activity_count")%>%
mutate(
    weekday_weekend = case_when(
      day == "Saturday" ~ "weekend",
      day == "Sunday" ~ "weekend",
      day == "Monday" ~ "weekday",
      day == "Tuesday" ~ "weekday",
      day == "Wednesday" ~ "weekday",
      day == "Thursday" ~ "weekday",
      day == "Friday" ~ "weekday",
      TRUE      ~ ""
  ))
```
Describe the dataset-

Number of observations in accel_df
```{r}
nrow(accel_df)
```


The cleaned dataset now has 6 variables. The week number and day number which are both coded as doubles, the day of the week which is a character variable, the minute of day which represents each minute in a 24 hour day, stored as a character, the activity count which is the physical activity measured by the accelerometer stored as a double, and weekday_weekend which depicts whether the day of the week is a weekday or weekend stored as a character variable. The final dataset has 50400 observations. 


Creating a table that aggregated activity count for each of the days 

```{r}
accel_df%>%
group_by(day_id)%>%
summarize(total_activity_count = sum(activity_count, na.rm = FALSE))%>%
knitr::kable()
  
```
There don't appear to be any major trends with activity count by day, and most of the values appear to be relatively random. However, there is a significant drop in physical activity measured on day 2, day 24, and day 31. Perhaps the participants did not have time to exercise those days due to some other commitments. 



Making a plot of 24 hour activity time courses for the different days.

```{r}
accel_df%>%
group_by(day_id)%>%
ggplot(aes(x = minute_of_day, y = activity_count, group = day_id, color = day))+
geom_line(alpha = 0.5)+
geom_smooth(aes(group = day))+
labs(title = "24-hour activity time course", x = "The minute of the day", y = "Activity count measured by accelerometer")
```

There is a clear spike in activity count in the early minutes of the 24 hour day and another smaller spike in activity count towards the end of the day. Perhaps the participants like exercising in the mornings and/or evenings. There is a lul in the middle hours of the day. 


#Problem 3

load the dataset

```{r}
data("ny_noaa")
```

clean the dataset

```{r}
ny_noaa_df = ny_noaa%>% 
janitor::clean_names()%>% 
separate(date, into = c("year","month","day"), convert = TRUE)%>%
mutate(prcp = as.numeric(prcp) / 10,
       snow = as.numeric(snow),
       snwd = as.numeric(snwd),
       tmin = as.numeric(tmin) / 10,
       tmax = as.numeric(tmax) / 10)
```

The ny_noaa data has been cleaned such that the date has separated into year, month, and day. The temperatures have been converted to degrees celsius from tenths of degrees celsius and all of the precipitation and snowfall have been converted to mm units. 

Now to look at the most common values of snowfall

```{r}
ny_noaa_df%>%
  ggplot(aes(x=snow))+
  geom_histogram(na.rm = TRUE)
```

From the histogram above, it is clear that the most common value by far is 0. This is because it makes sense that it only snows a few days in a year and hence there would be no snowfall recorded most of the year.

Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

```{r}
ny_noaa_df%>%
na.omit()%>%
filter(month %in% c(1,7))%>%
group_by(id,year,month)%>%
summarize(avg_tmax = mean(tmax))%>%
ggplot(  aes(x = year, y = avg_tmax, group = id, color = id) ) +
geom_point()+
geom_path()+
facet_grid(. ~  month)+
labs(title = "Average maximum mean temperature for January and July across stations and years", x = "year", y = "average maximum temperature (degrees C)") +
theme(legend.position = "none") 
      
```

When comparing the average max temperature between January and July, it is clear that the avergae max temperature is MUCH higher in July compared to January. Probably because summer temperatures are higher than winter temperatures. There don't appear to be any major outliers, but the average max temperature in the early 1980s and 1996 in Jan appear drastically low compared to the rest of the month and the same holds true for the late 1980s in July. Otherwise, the temperatures appear fairly consistent with only a little variability. 



Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

First, let's make the tmax vs tmin plot for the full dataset

```{r}
plot1_tmax_tmin = ny_noaa_df%>%
na.omit()%>%
ggplot(  aes(x = tmin, y = tmax) )+
geom_hex()+
labs(title = "Maximum temperature vs minimum temperature", x = "Minimum temperature (degrees C)", y = "Maximum temperature (degrees C)")
  
```

Second, let's make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r}

plot2_snowfall = ny_noaa_df%>%
na.omit()%>%
group_by(year)%>%
filter(snow > 0 & snow < 100)%>%
ggplot(  aes(x = year, y = snow, group = year) ) +
geom_boxplot(aes(fill = year))+
labs(title = "Snowfall distribution by year", x = "Year", y = "Snowfall in mm")
  


```


Use patchwork to stitch the two plots togther into a 2-panel plot

```{r}
plot1_tmax_tmin + plot2_snowfall
```

